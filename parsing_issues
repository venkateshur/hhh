Problems:

1. Since only record in the input, only one task is creating to process.
2. We are doing parsing the json having types Arrays and struct in loop, GC increasing
3. Only one partition is performing the parsing operation due to memory issues(memory utilization exceed limt set by yarn container), executor keep decommisioning.
4. Tried to extract Newtwork entities to create multiple records, but no uses. simple explode also failing


Soultions:
1. File should have multiple records instead of single record with bigger size
2. Other create cluster with worker node having 64GB(it may vary) and explode network object and write to output
3. Read output with multiple records and parse
